{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "1. http://docs.h2o.ai/h2o/latest-stable/h2o-docs/starting-h2o.html\n",
    "2. https://github.com/h2oai/h2o-tutorials/blob/master/tutorials/gbm-randomforest/GBM_RandomForest_Example.py\n",
    "3. https://blog.h2o.ai/2017/06/xgboost-in-h2o-machine-learning-platform/\n",
    "4. https://www.analyticsvidhya.com/blog/2016/05/h2o-data-table-build-models-large-data-sets/\n",
    "5. https://aichamp.wordpress.com/2017/10/19/calculating-auc-and-gini-model-metrics-for-logistic-classification/\n",
    "6. Parameter tuning: https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.ipynb\n",
    "7. Interpreting h2o predictions: https://stackoverflow.com/questions/45523997/how-should-we-interpret-the-results-of-the-h2o-predict-function\n",
    "8. Feature engineering: https://elitedatascience.com/feature-engineering-best-practices\n",
    "9. Imputing missing values: https://stackoverflow.com/questions/25239958/impute-categorical-missing-values-in-scikit-learn\n",
    "10. Beyond One-hot Encoding: http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/\n",
    "11. Encoding Categorical Variables: https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931; https://www.kaggle.com/gargmanish/how-to-handle-imbalance-data-study-in-detail\n",
    "12. Kaggle kernel on sampling data: https://www.kaggle.com/gargmanish/how-to-handle-imbalance-data-study-in-detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/araks/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import h2o\n",
    "import os\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Shape: (348978, 51) test Shape: (523466, 50)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data_transactions/train.csv')\n",
    "test = pd.read_csv('data_transactions/test.csv')\n",
    "\n",
    "print('train Shape:', train.shape, 'test Shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop id column\n",
    "train_new = train.copy()\n",
    "test_new = test.copy()\n",
    "\n",
    "id_train = train_new.transaction_id\n",
    "sub_ids = test_new.transaction_id\n",
    "\n",
    "train_new.drop('transaction_id', axis = 1, inplace = True)\n",
    "test_new.drop('transaction_id', axis = 1, inplace = True)\n",
    "\n",
    "#train_new.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Features With Only One Distinct Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348978, 42)\n",
      "(523466, 41)\n"
     ]
    }
   ],
   "source": [
    "cat_vars = [x for x in train_new.columns if 'cat_' in x]\n",
    "\n",
    "# in training set\n",
    "cat_to_drop_train = []\n",
    "for x in cat_vars:\n",
    "    if train_new[x].nunique() == 1:\n",
    "        cat_to_drop_train.append(x)\n",
    "\n",
    "# in test set\n",
    "cat_to_drop_test = []\n",
    "for x in cat_vars:\n",
    "    if test_new[x].nunique() == 1:\n",
    "        cat_to_drop_test.append(x)\n",
    "\n",
    "# drop these features\n",
    "cat_to_drop = list(set(cat_to_drop_train + cat_to_drop_test))\n",
    "train_new = train_new.drop(cat_to_drop, axis = 1)\n",
    "test_new = test_new.drop(cat_to_drop, axis = 1)\n",
    "\n",
    "print(train_new.shape)\n",
    "print(test_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348978, 42)\n",
      "(523466, 41)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_vars = [x for x in train_new.columns if 'cat_' in x]\n",
    "\n",
    "for x in cat_vars:\n",
    "    train_new[x] = train_new[x].fillna('NaN')\n",
    "    test_new[x] = test_new[x].fillna('NaN')\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(list(set(list(train_new[x]) + list(test_new[x]))))\n",
    "    train_new[x] = encoder.transform(train_new[x])\n",
    "    test_new[x] = encoder.transform(test_new[x])\n",
    "    \n",
    "print(train_new.shape)\n",
    "print(test_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with Imbalanced Data\n",
    "\n",
    "## Visualize *target* \n",
    "\n",
    "Fraud detection data are usualy very imbalanced (the number of normal classes is more than the number of fraud classes). The visual below shows the imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal(class 0): 0.892922, fraud(class 1): 0.107078\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEFCAYAAAAmIwo/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZv8PQGWX6e7yYW0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1349d6518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's check the distribution of target variable\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Look at the classes of target variable in percentages\n",
    "normal, fraud = train_new['target'].value_counts(normalize=True)\n",
    "print('normal(class 0): %f, fraud(class 1): %f' % (normal, fraud))\n",
    "\n",
    "ax = sns.countplot(\"target\", data = train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sampling\n",
    "\n",
    "We are going to do\n",
    "1. **Undersampling**, choosing all fraud transactions and only a portion of normal transactions.\n",
    "2. **Oversampling**, choosing all of the data (both fraud and normal), plus replicated fraud transactions.\n",
    "\n",
    "But befor doing the above two steps, let's define several helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function for undersampling  \n",
    "\n",
    "**Note**: We will call the validation objects by the name \"test\". Our real test set is the *test_new*, which we will keep until the end, when we will make predictions and submit answers for evaluation. \n",
    "\n",
    "1. Split the training set into train and test(validation) sets.  \n",
    "2. Undersample the train set only. This will assure that the distribution of test(validation) set is not changed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def undersample(train, response, p):    \n",
    "    # train: training data frame that needs to be undersampled\n",
    "    # response: name(string) of the response column\n",
    "    # p: float, portion of normal classes that we want to have in the final data set. \n",
    "    #    More specifically, normal = fraud * p/(1-p)\n",
    "    \n",
    "    X = train.drop(response, axis = 1)\n",
    "    y = train[response]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "    X_train = X_train.join(y_train)     # adding the response column to splitted train set\n",
    "    print(\"length of training data:\", len(X_train))\n",
    "    print(\"length of validation data:\", len(X_test))\n",
    "\n",
    "    train_normal = X_train[X_train[response] == 0]\n",
    "    train_fraud = X_train[X_train[response] == 1]\n",
    "    train_normal_under = train_normal.sample(n = int(round(len(train_fraud)*p/(1-p))), replace=False)\n",
    "    train_under = train_normal_under.append(train_fraud)\n",
    "    \n",
    "    print('total number of records in undersampled training set:', len(train_under))\n",
    "    print('portion of normal transactions in undersampled training set:', len(train_under[train_under[response]==0])/len(train_under))\n",
    "    print('portion of fraud transactions in undersampled training set:', len(train_under[train_under[response]==1])/len(train_under))\n",
    "    print('-----------------------------------------------------------------------------------------')\n",
    "    \n",
    "    X_train_under = train_under.drop(response, axis = 1)\n",
    "    y_train_under = train_under[response]\n",
    "    \n",
    "    return (X_train_under, X_test, y_train_under, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function for oversampling  \n",
    "\n",
    "1. Split the training into train and test(validation) sets.   \n",
    "2. Add copied fraud columns in train set only. This will assure that in our test(validation) set there are no fraud observations that we have already seen while training our model(s). We are not changing the distribution of test(validation) set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oversample(train, response, p):\n",
    "    # train: training data frame that needs to be oversampled\n",
    "    # response: name(string) of the response column\n",
    "    # p: float, the resulting fraud classes are less than p*total_rows\n",
    "    \n",
    "    X = train.drop(response, axis = 1)\n",
    "    y = train[response]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "    X_train = X_train.join(y_train)     # adding the response column to splitted train set\n",
    "    print(\"length of training data:\", len(X_train))\n",
    "    print(\"length of validation data:\", len(X_test))\n",
    "    \n",
    "    normal_data = X_train[X_train[response] == 0]\n",
    "    fraud_data = X_train[X_train[response] == 1]\n",
    "    total_fraud = len(X_train[X_train[response] == 1])\n",
    "    \n",
    "    while total_fraud < (len(normal_data))*p:\n",
    "        normal_data = normal_data.append(fraud_data)\n",
    "        total_fraud = len(normal_data[normal_data[response] == 1])\n",
    "        \n",
    "    train_over = normal_data.copy()\n",
    "    print('total number of records in oversampled training set:', len(train_over))\n",
    "    print('portion of normal transactions in oversampled training set:', len(train_over[train_over[response]==0])/len(train_over))\n",
    "    print('portion of fraud transactions in oversampled training set:', len(train_over[train_over[response]==1])/len(train_over))\n",
    "    print('----------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    X_train_over = train_over.drop(response, axis = 1)\n",
    "    y_train_over = train_over[response]\n",
    "    \n",
    "    return (X_train_over, X_test, y_train_over, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>1 day 19 hours 31 mins</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.16.0.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>13 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_araks_rzi2qe</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.294 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.5.4 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         1 day 19 hours 31 mins\n",
       "H2O cluster version:        3.16.0.2\n",
       "H2O cluster version age:    13 days\n",
       "H2O cluster name:           H2O_from_python_araks_rzi2qe\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.294 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.5.4 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### start h2o\n",
    "h2o.init()     #h2o.cluster().shutdown() # in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |███████████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### Convert Pandas Data Frame to H2o Frame\n",
    "#train_h2o = h2o.H2OFrame(train_new)   # do not convert train_new, as we will do it later after undersampling it\n",
    "test_h2o = h2o.H2OFrame(test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predictors and Response\n",
    "predictors_X = train_new.columns.values.tolist()\n",
    "response_y = 'target'\n",
    "\n",
    "### gbm object\n",
    "gbm = H2OGradientBoostingEstimator(\n",
    "    ntrees=50,\n",
    "    learn_rate=0.2,\n",
    "    max_depth=10,\n",
    "    #stopping_tolerance=0.01,\n",
    "    #stopping_rounds=2,\n",
    "    score_each_iteration=True,\n",
    "    model_id=\"gbm_covType_v2\",\n",
    "    seed=2000000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersample Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampled data for 0.2 proportion\n",
      "length of training data: 244284\n",
      "length of validation data: 104694\n",
      "total number of records in undersampled training set: 32656\n",
      "portion of normal transactions in undersampled training set: 0.19999387555120038\n",
      "portion of fraud transactions in undersampled training set: 0.8000061244487996\n",
      "-----------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "-------------------------- Start the Training ------------------------------\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "training score: 0.8741524287186836\n",
      "validation score: 0.7175923682603422\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Undersampled data for 0.3 proportion\n",
      "length of training data: 244284\n",
      "length of validation data: 104694\n",
      "total number of records in undersampled training set: 37380\n",
      "portion of normal transactions in undersampled training set: 0.3\n",
      "portion of fraud transactions in undersampled training set: 0.7\n",
      "-----------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "-------------------------- Start the Training ------------------------------\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "training score: 0.8535285362564438\n",
      "validation score: 0.7178095591423725\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Undersampled data for 0.4 proportion\n",
      "length of training data: 244284\n",
      "length of validation data: 104694\n",
      "total number of records in undersampled training set: 43850\n",
      "portion of normal transactions in undersampled training set: 0.4\n",
      "portion of fraud transactions in undersampled training set: 0.6\n",
      "-----------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "-------------------------- Start the Training ------------------------------\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "training score: 0.8495961795745577\n",
      "validation score: 0.7211005959505274\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Undersampled data for 0.5 proportion\n",
      "length of training data: 244284\n",
      "length of validation data: 104694\n",
      "total number of records in undersampled training set: 52434\n",
      "portion of normal transactions in undersampled training set: 0.5\n",
      "portion of fraud transactions in undersampled training set: 0.5\n",
      "-----------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "-------------------------- Start the Training ------------------------------\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "training score: 0.8264320828647982\n",
      "validation score: 0.7241408933474959\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Undersampled data for 0.6 proportion\n",
      "length of training data: 244284\n",
      "length of validation data: 104694\n",
      "total number of records in undersampled training set: 65187\n",
      "portion of normal transactions in undersampled training set: 0.5999969319036004\n",
      "portion of fraud transactions in undersampled training set: 0.40000306809639957\n",
      "-----------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "-------------------------- Start the Training ------------------------------\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "training score: 0.8202031327493363\n",
      "validation score: 0.7201808353337703\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Undersampled data for 0.7 proportion\n",
      "length of training data: 244284\n",
      "length of validation data: 104694\n",
      "total number of records in undersampled training set: 87187\n",
      "portion of normal transactions in undersampled training set: 0.7000011469599826\n",
      "portion of fraud transactions in undersampled training set: 0.29999885304001744\n",
      "-----------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "-------------------------- Start the Training ------------------------------\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "training score: 0.7953382132454484\n",
      "validation score: 0.7251146882789159\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Undersampled data for 0.8 proportion\n",
      "length of training data: 244284\n",
      "length of validation data: 104694\n",
      "total number of records in undersampled training set: 130555\n",
      "portion of normal transactions in undersampled training set: 0.8\n",
      "portion of fraud transactions in undersampled training set: 0.2\n",
      "-----------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "-------------------------- Start the Training ------------------------------\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "training score: 0.7775319315388012\n",
      "validation score: 0.724396167273473\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "### Do undersampling and fitting 4 times resulting in normal class portion of 0.25, 0.3, 0.4, and 0.5\n",
    "for i in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    print(\"Undersampled data for {} proportion\".format(i))\n",
    "    X_train_under, X_test_1, y_train_under, y_test_1 = undersample(train_new, 'target', i)\n",
    "    \n",
    "    ### For h2o we need the training and validation sets with the response column\n",
    "    ### So add the response column back\n",
    "    X_train_under = X_train_under.join(y_train_under)     \n",
    "    X_test_1 = X_test_1.join(y_test_1)\n",
    "    \n",
    "    ### Convert train and valid sets from pandas to h2o Frame\n",
    "    train_under = h2o.H2OFrame(X_train_under)\n",
    "    valid_under = h2o.H2OFrame(X_test_1)\n",
    "    \n",
    "    ### Convert the response column to factor\n",
    "    train_under['target'] = train_under['target'].asfactor()\n",
    "    valid_under['target'] = valid_under['target'].asfactor()\n",
    "    \n",
    "    ### Train\n",
    "    print(\"-------------------------- Start the Training ------------------------------\")\n",
    "    gbm.train(predictors_X, response_y, training_frame = train_under, validation_frame = valid_under)\n",
    "\n",
    "    ### Print the scores for training and validation sets\n",
    "    print('training score:', gbm.auc(train = True))\n",
    "    print('validation score:', gbm.auc(valid = True))   \n",
    "    print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversample Training\n",
    "\n",
    "This will take more than 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled data for 0.25 proportion\n",
      "length of training data: 244284\n",
      "length of validation data: 104694\n",
      "total number of records in oversampled training set: 296770\n",
      "portion of normal transactions in oversampled training set: 0.7347137513899653\n",
      "portion of fraud transactions in oversampled training set: 0.2652862486100347\n",
      "----------------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "-------------------------- Start the Training ------------------------------\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "training score: 0.8120509952612746\n",
      "validation score: 0.7222622464152813\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Oversampled data for 0.3 proportion\n",
      "length of training data: 244284\n",
      "length of validation data: 104694\n",
      "total number of records in oversampled training set: 322695\n",
      "portion of normal transactions in oversampled training set: 0.6760160523094563\n",
      "portion of fraud transactions in oversampled training set: 0.3239839476905437\n",
      "----------------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "-------------------------- Start the Training ------------------------------\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "training score: 0.8108852713804262\n",
      "validation score: 0.7192225036543963\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Oversampled data for 0.4 proportion\n",
      "length of training data: 244284\n",
      "length of validation data: 104694\n",
      "total number of records in oversampled training set: 375069\n",
      "portion of normal transactions in oversampled training set: 0.5815649920414644\n",
      "portion of fraud transactions in oversampled training set: 0.4184350079585356\n",
      "----------------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "-------------------------- Start the Training ------------------------------\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "training score: 0.8075637539350101\n",
      "validation score: 0.7138618510072927\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Oversampled data for 0.5 proportion\n",
      "length of training data: 244284\n",
      "length of validation data: 104694\n",
      "total number of records in oversampled training set: 453508\n",
      "portion of normal transactions in oversampled training set: 0.48098600245199646\n",
      "portion of fraud transactions in oversampled training set: 0.5190139975480036\n",
      "----------------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "-------------------------- Start the Training ------------------------------\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "training score: 0.8083981659388368\n",
      "validation score: 0.7165433953454763\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "### Do oversampling and fitting 4 times resulting in fraud class portion of 0.25, 0.3, 0.4, and 0.5\n",
    "for i in [0.25, 0.3, 0.4, 0.5]:\n",
    "    print(\"Oversampled data for {} proportion\".format(i))\n",
    "    X_train_over, X_test_2, y_train_over, y_test_2 = oversample(train_new, 'target', i)\n",
    "    \n",
    "    ### For h2o we need the training and validation sets with the response column\n",
    "    ### So add the response column back\n",
    "    X_train_over = X_train_over.join(y_train_over)     \n",
    "    X_test_2 = X_test_2.join(y_test_2)\n",
    "    \n",
    "    ### Convert train and valid sets from pandas to h2o Frame\n",
    "    train_over = h2o.H2OFrame(X_train_over)\n",
    "    valid_over = h2o.H2OFrame(X_test_2)\n",
    "    \n",
    "    ### Convert the response column to factor\n",
    "    train_over['target'] = train_over['target'].asfactor()\n",
    "    valid_over['target'] = valid_over['target'].asfactor()\n",
    "    \n",
    "    ### Train\n",
    "    print(\"-------------------------- Start the Training ------------------------------\")\n",
    "    gbm.train(predictors_X, response_y, training_frame = train_over, validation_frame = valid_over)\n",
    "\n",
    "    ### Print the scores for training and validation sets\n",
    "    print('training score:', gbm.auc(train = True))\n",
    "    print('validation score:', gbm.auc(valid = True))   \n",
    "    print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Test Data (Undersample)\n",
    "\n",
    "**Note**: Run the 4 cells bellow for undersampled data submission\n",
    "\n",
    "Choose the highest validation score and see what portion of normal class it corresponds to. Do undersampling again but this time on all of the data, do not split (do not use the function undersample).\n",
    "Change the portion (line 3) in the code below to the desired portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of records in undersampled training set: 46710\n",
      "portion of normal transactions in undersampled training set: 0.2\n",
      "portion of fraud transactions in undersampled training set: 0.8\n",
      "-----------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "train_normal = train_new[train_new['target'] == 0]\n",
    "train_fraud = train_new[train_new['target'] == 1]\n",
    "train_normal_under = train_normal.sample(n = int(round(len(train_fraud)*0.2/(1-0.2))), replace=False)\n",
    "train_under = train_normal_under.append(train_fraud)\n",
    "    \n",
    "print('total number of records in undersampled training set:', len(train_under))\n",
    "print('portion of normal transactions in undersampled training set:', len(train_under[train_under['target']==0])/len(train_under))\n",
    "print('portion of fraud transactions in undersampled training set:', len(train_under[train_under['target']==1])/len(train_under))\n",
    "print('-----------------------------------------------------------------------------------------')\n",
    "    \n",
    "train_h2o_under = h2o.H2OFrame(train_under)\n",
    "train_h2o_under['target'] = train_h2o_under['target'].asfactor()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### Train\n",
    "gbm.train(predictors_X, response_y, training_frame = train_h2o_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### Predict\n",
    "final_gbm_predictions_under = gbm.predict(test_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert h2o predictions to pandas data frame\n",
    "preds_pandas_under = h2o.as_list(final_gbm_predictions_under)\n",
    "# we need column 'p1' as it is the probability of target==1\n",
    "preds_under = preds_pandas_under['p1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Test Data (Oversample)\n",
    "\n",
    "**Note**: Run the 4 cells bellow for oversampled data submission\n",
    "\n",
    "Choose the highest validation score and see what portion of fraud class it corresponds to. Do oversampling again but this time on all of the data, do not split (do not use the function oversample).\n",
    "Change the portion in the code below to the desired portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of records in oversampled training set: 423714\n",
      "portion of normal transactions in oversampled training set: 0.735425310468854\n",
      "portion of fraud transactions in oversampled training set: 0.264574689531146\n",
      "----------------------------------------------------------------------------------------------\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "train_normal = train_new[train_new['target'] == 0]\n",
    "train_fraud = train_new[train_new['target'] == 1]\n",
    "train_fraud_count = len(train_fraud)\n",
    "\n",
    "while train_fraud_count < (len(train_normal))*0.25:\n",
    "        train_normal = train_normal.append(train_fraud)\n",
    "        train_fraud_count = len(train_normal[train_normal['target'] == 1])\n",
    "        \n",
    "train_over = train_normal.copy()\n",
    "print('total number of records in oversampled training set:', len(train_over))\n",
    "print('portion of normal transactions in oversampled training set:', len(train_over[train_over['target']==0])/len(train_over))\n",
    "print('portion of fraud transactions in oversampled training set:', len(train_over[train_over['target']==1])/len(train_over))\n",
    "print('----------------------------------------------------------------------------------------------')\n",
    " \n",
    "train_h2o_over = h2o.H2OFrame(train_over)\n",
    "train_h2o_over['target'] = train_h2o_over['target'].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### Train\n",
    "gbm.train(predictors_X, response_y, training_frame = train_h2o_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### Predict\n",
    "final_gbm_predictions_over = gbm.predict(test_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert h2o predictions to pandas data frame\n",
    "preds_pandas_over = h2o.as_list(final_gbm_predictions_over)\n",
    "# we need column 'p1' as it is the probability of target==1\n",
    "preds_over = preds_pandas_over['p1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "**Note**: Uncomment the first line if submiting the undersampled results and uncomment the second line for the oversampled results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = preds_under.copy()\n",
    "#preds = preds_over.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(523466,)\n",
      "(523466,)\n"
     ]
    }
   ],
   "source": [
    "print(sub_ids.shape)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='sub_transactions.csv' target='_blank'>sub_transactions.csv</a><br>"
      ],
      "text/plain": [
       "/Users/araks/Documents/AS_projects/Competitions/hackerearth/BrainWaves2017_18/sub_transactions.csv"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "sub = pd.DataFrame({'transaction_id': sub_ids, 'target': preds})\n",
    "sub = sub[['transaction_id','target']]    \n",
    "\n",
    "filename='sub_transactions.csv'\n",
    "sub.to_csv(filename, index=False)\n",
    "FileLink(filename)      # lb 0.72597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
